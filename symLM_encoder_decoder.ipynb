{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T17:41:52.735004Z",
     "iopub.status.busy": "2023-09-15T17:41:52.734250Z",
     "iopub.status.idle": "2023-09-15T17:41:54.212886Z",
     "shell.execute_reply": "2023-09-15T17:41:54.212009Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# torch.manual_seed(1337)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from models import GPTLanguageModel, SymGPTLanguageModel, FeedForwardSeqModel, SymLMEncoder\n",
    "from utils import get_batch_, estimate_loss_, calculate_cooc\n",
    "import string\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T17:41:54.219452Z",
     "iopub.status.busy": "2023-09-15T17:41:54.219198Z",
     "iopub.status.idle": "2023-09-15T17:41:54.223633Z",
     "shell.execute_reply": "2023-09-15T17:41:54.223122Z"
    }
   },
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "batch_size = 64\n",
    "max_iters = 2000\n",
    "eval_interval = 50\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "dropout = 0.2\n",
    "\n",
    "# model configuration\n",
    "block_size = n_head = 6\n",
    "n_embd = 128\n",
    "n_layer = 1\n",
    "\n",
    "# encoding config\n",
    "lowercase_only = True\n",
    "intermediate_vocab_size_f = [1.5]  # out_vocab_size = int(f*in_vocab_size)\n",
    "                                        # f=1.0 for output vocabulary be equal to the input vocabulary size\n",
    "f_seed = 1.0                            # fraction of tokens to be used for retokenization clustering   \n",
    "self_loops = True              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T17:41:54.228927Z",
     "iopub.status.busy": "2023-09-15T17:41:54.228761Z",
     "iopub.status.idle": "2023-09-15T17:41:54.232283Z",
     "shell.execute_reply": "2023-09-15T17:41:54.231770Z"
    }
   },
   "outputs": [],
   "source": [
    "outdir = './res/res_encoding_decoding_context_win_{}_vocab_f_{}_cluster_f_{}_{}_self_loops'.format(n_head, \n",
    "                                                                                    '-'.join([str(f) for f in intermediate_vocab_size_f]), \n",
    "                                                                                    f_seed,\n",
    "                                                                                    'w' if self_loops else 'wo')\n",
    "os.makedirs(outdir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T17:41:54.237336Z",
     "iopub.status.busy": "2023-09-15T17:41:54.237112Z",
     "iopub.status.idle": "2023-09-15T17:41:54.252606Z",
     "shell.execute_reply": "2023-09-15T17:41:54.251744Z"
    }
   },
   "outputs": [],
   "source": [
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('./data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "if lowercase_only:\n",
    "    text = text.lower()\n",
    "chars_in_text = sorted(list(set(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes: \n",
    "1. The pipeline now uses the character encoding as indexing! This means that we should ALWAYS start the character encoding with 0, [0, vocab_out_size-1]\n",
    "2. Each re-write ignores the first block of tokens because token re-writes are performed using the context of the previous tokens in the block, mean the first block has no context. As a result, the current behavior is that each re-write \"forgets\" the first tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T17:41:54.257448Z",
     "iopub.status.busy": "2023-09-15T17:41:54.257232Z",
     "iopub.status.idle": "2023-09-15T17:41:54.263324Z",
     "shell.execute_reply": "2023-09-15T17:41:54.262655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max vocabulary size:  100\n",
      "Complete vocabulary:  ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\"', '#', '%', '(', ')', '*', '+', '/', '<', '=', '>', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\t', '\\r', '\\x0b', '\\x0c']\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from all printable characters to integers\n",
    "chars = [chr(i) for i in range(128)]    # all ascii characters\n",
    "chars = [i for i in string.printable]   # printable characters\n",
    "chars = chars_in_text + [i for i in string.printable if i not in chars_in_text]   # the initial characters should be indexed first\n",
    "target_vocab_size = len(chars_in_text)\n",
    "max_vocab_size = len(chars)\n",
    "print(\"Max vocabulary size: \", max_vocab_size)\n",
    "print(\"Complete vocabulary: \", chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T17:41:54.313460Z",
     "iopub.status.busy": "2023-09-15T17:41:54.313118Z",
     "iopub.status.idle": "2023-09-15T17:41:54.412344Z",
     "shell.execute_reply": "2023-09-15T17:41:54.411683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chorpus size: 1003854\n"
     ]
    }
   ],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(\"Chorpus size: {}\".format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T17:41:54.427309Z",
     "iopub.status.busy": "2023-09-15T17:41:54.427005Z",
     "iopub.status.idle": "2023-09-15T18:00:24.199049Z",
     "shell.execute_reply": "2023-09-15T18:00:24.198247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocabulary size: 39\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/rissakiagapi/sym-llm/symLM_encoder_decoder.ipynb Cell 9\u001b[0m line \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvldb.khoury.northeastern.edu/home/rissakiagapi/sym-llm/symLM_encoder_decoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m symbolic_encoder \u001b[39m=\u001b[39m SymLMEncoder(vocab_out_size, device, cooc, self_loops\u001b[39m=\u001b[39mself_loops)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvldb.khoury.northeastern.edu/home/rissakiagapi/sym-llm/symLM_encoder_decoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# context-aware representation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvldb.khoury.northeastern.edu/home/rissakiagapi/sym-llm/symLM_encoder_decoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m data_enc_rep, _ \u001b[39m=\u001b[39m symbolic_encoder(torch\u001b[39m.\u001b[39;49mstack([data_in[i:i\u001b[39m+\u001b[39;49mblock_size] \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(data_in) \u001b[39m-\u001b[39;49m block_size)]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvldb.khoury.northeastern.edu/home/rissakiagapi/sym-llm/symLM_encoder_decoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m data_val_enc_rep, _ \u001b[39m=\u001b[39m symbolic_encoder(torch\u001b[39m.\u001b[39mstack([data_val_in[i:i\u001b[39m+\u001b[39mblock_size] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data_val_in) \u001b[39m-\u001b[39m block_size)]))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvldb.khoury.northeastern.edu/home/rissakiagapi/sym-llm/symLM_encoder_decoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIntermediate representation size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(it\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, data_enc_rep\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/neuro/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/sym-llm/models.py:343\u001b[0m, in \u001b[0;36mSymLMEncoder.forward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    341\u001b[0m x \u001b[39m=\u001b[39m idx\u001b[39m.\u001b[39mreshape((B, T, \u001b[39m1\u001b[39m)) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39m# produce context aware intermediate representation\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msym_attn(x) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39m# focus only on the last time step, \u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# the last token uses context from the whole context window/block\u001b[39;00m\n\u001b[1;32m    346\u001b[0m x_rep \u001b[39m=\u001b[39m x[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :] \u001b[39m# becomes (B, C)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuro/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/sym-llm/models.py:253\u001b[0m, in \u001b[0;36mSymbolicMultiHeadAttentionEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 253\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (B, T, hs*T)\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/sym-llm/models.py:253\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 253\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (B, T, hs*T)\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/neuro/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/sym-llm/models.py:206\u001b[0m, in \u001b[0;36mSymbolicAttentionHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    204\u001b[0m         att_scores[b] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m self_loops\n\u001b[1;32m    205\u001b[0m \u001b[39m# Perform the weighted aggregation of the values\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(x\u001b[39m.\u001b[39;49msqueeze()) \u001b[39m# (B,T,hs)\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m# Compute the context-aware representations\u001b[39;00m\n\u001b[1;32m    208\u001b[0m out \u001b[39m=\u001b[39m att_scores \u001b[39m@\u001b[39m v \u001b[39m# (B, T, T) @ (B, T, hs) -> (B, T, hs)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/neuro/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/neuro/lib/python3.9/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/neuro/lib/python3.9/site-packages/torch/nn/functional.py:2199\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2193\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2194\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2198\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2199\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "# for each encoding layer, \n",
    "# 1. calculate the cooc statistics from the prev corpus\n",
    "# 2. define the symbolic attention\n",
    "# 3. re-write the corpus into context-aware symbols\n",
    "vocab_in = list(set(data.tolist()))\n",
    "vocab_size_initial = len(vocab_in)\n",
    "print(\"Input vocabulary size: {}\".format(len(vocab_in)))\n",
    "data_in = train_data.to(device)\n",
    "data_val_in = val_data.to(device)\n",
    "# for f in intermediate_vocab_size_f:\n",
    "for it,f in enumerate(intermediate_vocab_size_f):\n",
    "    ## Preprocessing\n",
    "    # Co-occurence matrix calculation\n",
    "    if it>0: # skip the initial statistic, only calculate and save the intermediate ones\n",
    "        cooc = calculate_cooc(data_in, len(vocab_in), n_head, device=device)\n",
    "        for h in range(n_head):\n",
    "            plt.imshow(cooc[:,:,h].cpu().numpy())\n",
    "            plt.savefig('{}/{}-cooc_{}.png'.format(outdir, h+1, f))\n",
    "    elif os.path.exists('./data/cooc_{}.pth'.format(n_head)):\n",
    "        cooc = torch.load('./data/cooc_{}.pth'.format(n_head)).to(device)\n",
    "    else:\n",
    "        cooc = calculate_cooc(data_in, len(vocab_in), n_head, device=device)\n",
    "        torch.save(cooc.cpu(), './data/cooc_{}.pth'.format(n_head))\n",
    "    ## Encode corpus\n",
    "    # define the symbolic encoder\n",
    "    vocab_out_size = min(int(f*vocab_size_initial), max_vocab_size)\n",
    "    symbolic_encoder = SymLMEncoder(vocab_out_size, device, cooc, self_loops=self_loops)\n",
    "    # context-aware representation\n",
    "    data_enc_rep, _ = symbolic_encoder(torch.stack([data_in[i:i+block_size] for i in range(len(data_in) - block_size)]))\n",
    "    data_val_enc_rep, _ = symbolic_encoder(torch.stack([data_val_in[i:i+block_size] for i in range(len(data_val_in) - block_size)]))\n",
    "    print(\"Intermediate representation size {}: {}\".format(it+1, data_enc_rep.shape[1]))\n",
    "    # clustering\n",
    "    data_enc_rep = data_enc_rep.numpy()\n",
    "    data_val_enc_rep = data_val_enc_rep.numpy()\n",
    "    n_seed = min(int(f_seed*len(data_enc_rep)), len(data_enc_rep))\n",
    "    seed_idx = sample(list(range(len(data_enc_rep))), n_seed)\n",
    "    data_enc_rep_seed = data_enc_rep[seed_idx]\n",
    "    # encoding\n",
    "    kmeans = KMeans(n_clusters=vocab_out_size).fit(data_enc_rep_seed)\n",
    "    data_enc = kmeans.predict(data_enc_rep)\n",
    "    data_val_enc = kmeans.predict(data_val_enc_rep)\n",
    "    print(\"Intermediate vocabulary size {}: {}\".format(it+1, vocab_out_size))\n",
    "    ## Reset\n",
    "    vocab_in = list(range(vocab_out_size))\n",
    "    data_in = torch.tensor(data_enc, dtype=torch.long).to(device)\n",
    "    data_val_in = torch.tensor(data_val_enc, dtype=torch.long).to(device)\n",
    "\n",
    "vocab_size = len(vocab_in)\n",
    "train_data_enc = data_in\n",
    "val_data_enc = data_val_in\n",
    "offset = block_size*len(intermediate_vocab_size_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T18:00:24.210708Z",
     "iopub.status.busy": "2023-09-15T18:00:24.210343Z",
     "iopub.status.idle": "2023-09-15T18:00:24.289189Z",
     "shell.execute_reply": "2023-09-15T18:00:24.288358Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dict = {'train': (train_data_enc, train_data[offset:]), 'val': (val_data_enc, val_data[offset:])}\n",
    "assert len(train_data_enc)==len(train_data[offset:])\n",
    "assert len(val_data_enc)==len(val_data[offset:])\n",
    "\n",
    "\n",
    "text_encoded = decode(train_data_enc.tolist())\n",
    "with open('{}/latent_input.txt'.format(outdir), 'w') as f:\n",
    "    f.write(''.join(text_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Re-write Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocessing: Co-occurence matrix calculation\n",
    "# cooc = calculate_cooc(data_dict['train'][0], vocab_size, n_head, device=device) \n",
    "# # save the cooc statistics\n",
    "# for h in range(n_head):\n",
    "#     plt.imshow(cooc[:,:,h].cpu().numpy())\n",
    "#     plt.savefig('{}/{}-cooc_{}.png'.format(outdir, h+1, intermediate_vocab_size_f[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T18:00:24.295460Z",
     "iopub.status.busy": "2023-09-15T18:00:24.295267Z",
     "iopub.status.idle": "2023-09-15T18:00:24.298719Z",
     "shell.execute_reply": "2023-09-15T18:00:24.298219Z"
    }
   },
   "outputs": [],
   "source": [
    "model_types = ['symbolic'] #, 'original', 'baseline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-15T18:00:24.304777Z",
     "iopub.status.busy": "2023-09-15T18:00:24.304522Z",
     "iopub.status.idle": "2023-09-15T18:13:17.833701Z",
     "shell.execute_reply": "2023-09-15T18:13:17.832882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From SymbolicMultiHeadAttention: N 58, K 6, hs 348, embedding_dim 128\n",
      "======>\n",
      "Model type:  symbolic\n",
      "0.525751 M parameters\n",
      "Model:  SymGPTLanguageModel(\n",
      "  (blocks): Sequential(\n",
      "    (0): SymbolicBlock(\n",
      "      (sa): SymbolicMultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0): SymbolicAttentionHead(\n",
      "            (value): Embedding(58, 348)\n",
      "          )\n",
      "          (1): SymbolicAttentionHead(\n",
      "            (value): Embedding(58, 348)\n",
      "          )\n",
      "          (2): SymbolicAttentionHead(\n",
      "            (value): Embedding(58, 348)\n",
      "          )\n",
      "          (3): SymbolicAttentionHead(\n",
      "            (value): Embedding(58, 348)\n",
      "          )\n",
      "          (4): SymbolicAttentionHead(\n",
      "            (value): Embedding(58, 348)\n",
      "          )\n",
      "          (5): SymbolicAttentionHead(\n",
      "            (value): Embedding(58, 348)\n",
      "          )\n",
      "        )\n",
      "        (proj): Linear(in_features=2088, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (ffwd): FeedFoward(\n",
      "        (net): Sequential(\n",
      "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (3): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=128, out_features=39, bias=True)\n",
      ")\n",
      "step 0: train loss 3.6776, val loss 3.6793\n",
      "step 50: train loss 3.0297, val loss 3.0343\n",
      "step 100: train loss 2.9388, val loss 2.9365\n",
      "step 150: train loss 2.8936, val loss 2.8910\n",
      "step 200: train loss 2.8618, val loss 2.8656\n",
      "step 250: train loss 2.8585, val loss 2.8571\n",
      "step 300: train loss 2.8478, val loss 2.8468\n",
      "step 350: train loss 2.8308, val loss 2.8406\n",
      "step 400: train loss 2.8190, val loss 2.8319\n",
      "step 450: train loss 2.8337, val loss 2.8346\n",
      "step 500: train loss 2.8117, val loss 2.8239\n",
      "step 550: train loss 2.8093, val loss 2.8242\n",
      "step 600: train loss 2.7951, val loss 2.8125\n",
      "step 650: train loss 2.7982, val loss 2.8092\n",
      "step 700: train loss 2.8027, val loss 2.8156\n",
      "step 750: train loss 2.7870, val loss 2.8011\n",
      "step 800: train loss 2.7964, val loss 2.8057\n",
      "step 850: train loss 2.7867, val loss 2.7996\n",
      "step 900: train loss 2.7860, val loss 2.8084\n",
      "step 950: train loss 2.7973, val loss 2.8183\n",
      "step 1000: train loss 2.7844, val loss 2.7938\n",
      "step 1050: train loss 2.7763, val loss 2.7926\n",
      "step 1100: train loss 2.7781, val loss 2.8028\n",
      "step 1150: train loss 2.7882, val loss 2.7977\n",
      "step 1200: train loss 2.7914, val loss 2.7910\n",
      "step 1250: train loss 2.7806, val loss 2.7866\n",
      "step 1300: train loss 2.7860, val loss 2.8032\n",
      "step 1350: train loss 2.7746, val loss 2.7968\n",
      "step 1400: train loss 2.7824, val loss 2.7969\n",
      "step 1450: train loss 2.7735, val loss 2.7852\n",
      "step 1500: train loss 2.7732, val loss 2.7950\n",
      "step 1550: train loss 2.7740, val loss 2.7833\n",
      "step 1600: train loss 2.7746, val loss 2.7911\n",
      "step 1650: train loss 2.7666, val loss 2.7912\n",
      "step 1700: train loss 2.7721, val loss 2.7895\n",
      "step 1750: train loss 2.7707, val loss 2.7968\n",
      "step 1800: train loss 2.7704, val loss 2.7819\n",
      "step 1850: train loss 2.7705, val loss 2.7881\n",
      "step 1900: train loss 2.7612, val loss 2.7752\n",
      "step 1950: train loss 2.7653, val loss 2.7825\n",
      "step 1999: train loss 2.7675, val loss 2.7789\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_type in model_types:\n",
    "    if model_type=='symbolic':\n",
    "        # preprocessing: Co-occurence matrix calculation\n",
    "        cooc = calculate_cooc(data_dict['train'][0], vocab_size, n_head, device=device) \n",
    "        # save the cooc statistics\n",
    "        torch.save(cooc.cpu(), '{}/cooc_{}.pth'.format(outdir, intermediate_vocab_size_f[-1]))\n",
    "        for h in range(n_head):\n",
    "            plt.imshow(cooc[:,:,h].cpu().numpy())\n",
    "            plt.savefig('{}/{}-cooc_{}.png'.format(outdir, h+1, intermediate_vocab_size_f[-1]))\n",
    "        model = SymGPTLanguageModel(target_vocab_size, n_embd, n_head, n_layer, block_size, dropout, device, cooc)  \n",
    "    elif model_type=='original':\n",
    "        model = GPTLanguageModel(target_vocab_size, n_embd, n_head, n_layer, block_size, dropout, device)\n",
    "    elif model_type=='baseline':\n",
    "        model = FeedForwardSeqModel(target_vocab_size, n_embd, n_head, n_layer, block_size, dropout, device)\n",
    "    else:\n",
    "        assert False, \"Undefined model type.\"\n",
    "    model = model.to(device)\n",
    "    print('======>\\nModel type: ', model_type)\n",
    "    # print the number of parameters in the model\n",
    "    print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "    # print model summary\n",
    "    print('Model: ', model)\n",
    "\n",
    "    # create a PyTorch optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # bookkeeping\n",
    "    losses_log = dict({'iter': [], 'train': [], 'val': []})\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss_(model, data_dict, eval_iters, block_size, batch_size, device=device)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "            losses_log['train'].append(losses['train'].item())\n",
    "            losses_log['val'].append(losses['val'].item())\n",
    "            losses_log['iter'].append(iter)\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch_(data_dict, 'train', block_size, batch_size, device)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # save losses\n",
    "    with open('{}/losses_{}.json'.format(outdir, model_type), 'w') as fp:\n",
    "        json.dump(losses_log, fp)\n",
    "\n",
    "    # save model\n",
    "    torch.save(model.state_dict(), '{}/gpt_{}.pth'.format(outdir, model_type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('neuro')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "45a9a558b30b86d9f732c54dbfd32f3dc135cf8debc1699b6136107345de1818"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
